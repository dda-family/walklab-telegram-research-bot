import os
import re
import html
import feedparser
import requests
from datetime import datetime, timedelta, timezone
from dateutil import parser as dateparser
from urllib.parse import urlparse, parse_qs

# =============================
# ê¸°ë³¸ ì„¤ì •
# =============================
MAX_ARTICLES = 10
TIME_WINDOW_HOURS = 48

KST = timezone(timedelta(hours=9))
now_kst = datetime.now(KST)
cutoff_kst = now_kst - timedelta(hours=TIME_WINDOW_HOURS)

TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
    raise RuntimeError("Missing TELEGRAM_BOT_TOKEN or TELEGRAM_CHAT_ID in environment variables.")

# =============================
# RSS ëª©ë¡ (ê²½ìŸì‚¬ + íŠ¸ë Œë“œ + ì‹¤ë£¨ì—£/ìì„¸ + EverEx)
# =============================
RSS_FEEDS = [
    # ===== ê²½ìŸì‚¬ =====
    # AIT Studio / MediStep
    'https://news.google.com/rss/search?q=("AIT+Studio"+OR+AITìŠ¤íŠœë””ì˜¤+OR+ì—ì´íŠ¸ìŠ¤íŠœë””ì˜¤)+(MediStep+OR+ë©”ë””ìŠ¤í…)+(gait+OR+ë³´í–‰)&hl=ko&gl=KR&ceid=KR:ko',

    # Angel Robotics
    'https://news.google.com/rss/search?q=("Angel+Robotics"+OR+ì—”ì ¤ë¡œë³´í‹±ìŠ¤)+("Angel+Legs"+OR+M20)+(gait+OR+rehabilitation)&hl=ko&gl=KR&ceid=KR:ko',

    # WIRobotics
    'https://news.google.com/rss/search?q=("WIRobotics"+OR+ìœ„ë¡œë³´í‹±ìŠ¤)+(gait+OR+ì›¨ì–´ëŸ¬ë¸”)&hl=ko&gl=KR&ceid=KR:ko',

    # Spina Systems / PediSol
    'https://news.google.com/rss/search?q=(PediSol+OR+í˜ë””ì†”+OR+"Spina+Systems")+("smart+insole"+OR+ì¡±ì €ì••)&hl=ko&gl=KR&ceid=KR:ko',

    # Ochy
    'https://news.google.com/rss/search?q=(Ochy)+(gait)&hl=en-US&gl=US&ceid=US:en',

    # ExaMD / LocoStep
    'https://news.google.com/rss/search?q=("LocoStep"+OR+"ExaMD")+(gait)&hl=en-US&gl=US&ceid=US:en',

    # OneStep
    'https://news.google.com/rss/search?q=("OneStep")+(gait+OR+rehabilitation)&hl=en-US&gl=US&ceid=US:en',

    # ===== ì¶”ê°€ ê²½ìŸì‚¬: EverEx (êµ­ë¬¸ 2 + ì˜ë¬¸ 2) =====
    # ì „ëµ ì´ë²¤íŠ¸ ê°ì§€ (KR)
    'https://news.google.com/rss/search?q=("ì—ë²„ì—‘ìŠ¤"+OR+"EverEx")+(íˆ¬ì+OR+ì‹œë¦¬ì¦ˆ+OR+ì„ìƒ+OR+ë³‘ì›+OR+MOU+OR+ì œíœ´+OR+ë³´í—˜+OR+ìˆ˜ê°€+OR+ë””ì§€í„¸ì¹˜ë£Œê¸°ê¸°+OR+DTx+OR+ê³¼ì œ+OR+í•´ì™¸ì§„ì¶œ)&hl=ko&gl=KR&ceid=KR:ko',
    # ê¸°ìˆ /ì œí’ˆ ë°©í–¥ì„± (KR)
    'https://news.google.com/rss/search?q=("ì—ë²„ì—‘ìŠ¤"+OR+"EverEx")+(AI+OR+ì˜ìƒ+OR+ë¹„ì „+OR+ë¶„ì„+OR+ìš´ë™ì½”ì¹­+OR+ì¬í™œí”Œë«í¼+OR+ìì„¸+OR+ì‹¤ë£¨ì—£+OR+êµ°ì§‘)&hl=ko&gl=KR&ceid=KR:ko',
    # ì „ëµ ì´ë²¤íŠ¸ ê°ì§€ (EN)
    'https://news.google.com/rss/search?q=("EverEx")+(funding+OR+investment+OR+clinical+OR+hospital+OR+insurance+OR+DTx+OR+expansion+OR+partnership)&hl=en-US&gl=US&ceid=US:en',
    # ê¸°ìˆ  ë°©í–¥ì„± (EN)
    'https://news.google.com/rss/search?q=("EverEx")+("digital+rehabilitation"+OR+"AI+therapy"+OR+"motion+analysis"+OR+"pose+estimation"+OR+"exercise+platform")&hl=en-US&gl=US&ceid=US:en',

    # ===== íŠ¸ë Œë“œ 4ì¶• =====
    # ì˜ìƒ ê¸°ë°˜ ì„ìƒ/ê²€ì¦
    'https://news.google.com/rss/search?q=("smartphone+video+gait"+OR+"video+gait+analysis")+(clinical+OR+validation)&hl=en-US&gl=US&ceid=US:en',
    # ë³´í—˜ / ë¦¬ìŠ¤í¬
    'https://news.google.com/rss/search?q=("gait+digital+biomarker"+OR+"mobility+data")+(insurance+OR+underwriting)&hl=en-US&gl=US&ceid=US:en',
    # ê³ ë ¹ì ë‚™ìƒ
    'https://news.google.com/rss/search?q=("fall+prevention"+OR+"fall+risk")+(elderly+OR+seniors)&hl=en-US&gl=US&ceid=US:en',
    # ì§ˆí™˜ ì˜ˆì¸¡
    'https://news.google.com/rss/search?q=("gait+diabetes"+OR+"gait+Parkinson")+(AI+OR+model)&hl=en-US&gl=US&ceid=US:en',

    # ===== ì¶”ê°€: ìì„¸(ì‹¤ë£¨ì—£) 1ì¶•(ì˜/êµ­ë¬¸ í¬í•¨) =====
    'https://news.google.com/rss/search?q=("silhouette+analysis"+OR+"silhouette-based"+OR+"silhouette+score"+OR+clustering)+("posture"+OR+"pose+estimation"+OR+"motion+analysis"+OR+biomechanics)+(gait+OR+rehabilitation+OR+healthcare+OR+clinical)&hl=en-US&gl=US&ceid=US:en',
    'https://news.google.com/rss/search?q=(ì‹¤ë£¨ì—£+OR+ìì„¸+OR+í¬ì¦ˆì¶”ì •+OR+êµ°ì§‘ë¶„ì„)+(ë³´í–‰+OR+ì¬í™œ+OR+í—¬ìŠ¤ì¼€ì–´+OR+ì˜ë£Œ)+-íŒ¨ì…˜+-ì˜ë¥˜+-ì‚¬ì§„&hl=ko&gl=KR&ceid=KR:ko',
]

# =============================
# íƒœê·¸ ë¶„ë¥˜
# =============================
COMPANY_KEYWORDS = [
    "AIT Studio", "AITìŠ¤íŠœë””ì˜¤", "ì—ì´íŠ¸ìŠ¤íŠœë””ì˜¤", "MediStep", "ë©”ë””ìŠ¤í…",
    "Angel Robotics", "ì—”ì ¤ë¡œë³´í‹±ìŠ¤", "Angel Legs", "M20",
    "WIRobotics", "ìœ„ë¡œë³´í‹±ìŠ¤",
    "Spina Systems", "ìŠ¤í”¼ë‚˜ì‹œìŠ¤í…œì¦ˆ", "PediSol", "í˜ë””ì†”",
    "Ochy", "LocoStep", "ExaMD", "OneStep",
    "EverEx", "ì—ë²„ì—‘ìŠ¤"
]

TAG_RULES = {
    "ğŸ’°íˆ¬ì": ["funding", "series", "investment", "raises", "íˆ¬ì", "ì‹œë¦¬ì¦ˆ"],
    "ğŸ¤ì œíœ´": ["partnership", "collaboration", "mou", "ì œíœ´", "í˜‘ì•½", "mou"],
    "ğŸ¥ì„ìƒ": ["clinical", "trial", "fda", "validation", "hospital", "ì„ìƒ", "ë³‘ì›"],
    "ğŸ›ê³µê³µ": ["government", "city", "public", "ì •ë¶€", "ì§€ìì²´", "ê³µê³µ"],
    "ğŸ›¡ë³´í—˜": ["insurance", "underwriting", "payer", "ë³´í—˜", "ìˆ˜ê°€"],
    "ğŸ“±ì˜ìƒê¸°ë°˜": ["smartphone", "video", "camera", "markerless", "ì˜ìƒ", "ë¹„ì „", "ì¹´ë©”ë¼"],
    # êµ°ì§‘/ì‹¤ë£¨ì—£/ìì„¸ ê´€ë ¨ì€ ëª¨ë‘ ğŸ§ì‹¤ë£¨ì—£ í•˜ë‚˜ë¡œ í†µí•©
    "ğŸ§ì‹¤ë£¨ì—£": [
        "silhouette analysis", "silhouette-based", "silhouette score",
        "clustering", "cluster analysis",
        "posture", "pose estimation", "biomechanics",
        "ì‹¤ë£¨ì—£", "ìì„¸", "í¬ì¦ˆì¶”ì •", "êµ°ì§‘ë¶„ì„"
    ]
}

def send_telegram(message: str):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }
    r = requests.post(url, data=payload, timeout=20)
    r.raise_for_status()

def normalize_title(title: str) -> str:
    return re.sub(r"[^0-9a-zA-Zê°€-í£]+", "", title).lower()

def parse_published_kst(entry):
    if hasattr(entry, "published"):
        try:
            dt = dateparser.parse(entry.published)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt.astimezone(KST)
        except Exception:
            return None
    return None

def classify_tags(title: str, summary: str):
    text = (title + " " + summary).lower()
    tags = []

    for kw in COMPANY_KEYWORDS:
        if kw.lower() in text:
            tags.append("ğŸ¢ê²½ìŸì‚¬")
            break

    for tag, kws in TAG_RULES.items():
        for k in kws:
            if k.lower() in text:
                tags.append(tag)
                break

    return tags

def calc_priority(tags):
    score = 0
    if "ğŸ¢ê²½ìŸì‚¬" in tags:
        score += 100
    if "ğŸ’°íˆ¬ì" in tags:
        score += 30
    if "ğŸ¥ì„ìƒ" in tags:
        score += 30
    if "ğŸ¤ì œíœ´" in tags:
        score += 20
    if "ğŸ›¡ë³´í—˜" in tags:
        score += 15
    if "ğŸ›ê³µê³µ" in tags:
        score += 15
    if "ğŸ§ì‹¤ë£¨ì—£" in tags:
        score += 10
    if "ğŸ“±ì˜ìƒê¸°ë°˜" in tags:
        score += 8
    return score

def extract_original_url(url: str) -> str:
    """
    Google News RSS ë§í¬ì— url= íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì›ë¬¸ URLë¡œ ë°”ê¿‰ë‹ˆë‹¤.
    (ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜)
    """
    try:
        parsed = urlparse(url)
        qs = parse_qs(parsed.query)
        if "url" in qs and qs["url"]:
            return qs["url"][0]
    except Exception:
        pass
    return url

def format_item(i: int, tags, title: str, link: str) -> str:
    safe_title = html.escape(title)
    safe_link = html.escape(extract_original_url(link))
    tag_text = " ".join(tags) if tags else ""
    # ì œëª© í´ë¦­í˜• ë§í¬: ê¸´ URLì„ ë©”ì‹œì§€ì— ë…¸ì¶œí•˜ì§€ ì•ŠìŒ
    return f"{i}. {tag_text}\n<a href=\"{safe_link}\">{safe_title}</a>\n"

def main():
    articles = []
    seen_links = set()
    seen_titles = set()

    for feed_url in RSS_FEEDS:
        feed = feedparser.parse(feed_url)

        for entry in getattr(feed, "entries", []):
            published_kst = parse_published_kst(entry)
            if not published_kst:
                continue
            if published_kst < cutoff_kst:
                continue

            link = getattr(entry, "link", "").strip()
            title = getattr(entry, "title", "").strip()
            summary = getattr(entry, "summary", "")

            if not link or not title:
                continue

            norm_title = normalize_title(title)

            # ì¤‘ë³µ ì œê±° (ë§í¬ + ì œëª©)
            if link in seen_links or norm_title in seen_titles:
                continue

            seen_links.add(link)
            seen_titles.add(norm_title)

            tags = classify_tags(title, summary)
            priority = calc_priority(tags)

            articles.append({
                "title": title,
                "link": link,
                "tags": tags,
                "priority": priority,
                "published_kst": published_kst
            })

    # ì „ì²´ ìš°ì„ ìˆœìœ„ ì •ë ¬ í›„ ìƒìœ„ 10ê±´ë§Œ ì»·
    articles.sort(key=lambda x: (x["priority"], x["published_kst"]), reverse=True)
    top = articles[:MAX_ARTICLES]

    if not top:
        send_telegram("ğŸ“¡ ì˜¤ëŠ˜ ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ (ìµœê·¼ 48ì‹œê°„ ê¸°ì¤€)")
        return

    # êµ¬ì—­ ë¶„ë¦¬(ìƒìœ„ 10ê±´ ì•ˆì—ì„œë§Œ)
    competitors = [a for a in top if "ğŸ¢ê²½ìŸì‚¬" in a["tags"]]
    trends = [a for a in top if "ğŸ¢ê²½ìŸì‚¬" not in a["tags"]]

    # ì„¹ì…˜ ë‚´ì—ì„œë„ ìš°ì„ ìˆœìœ„/ìµœì‹ ìˆœ ìœ ì§€
    competitors.sort(key=lambda x: (x["priority"], x["published_kst"]), reverse=True)
    trends.sort(key=lambda x: (x["priority"], x["published_kst"]), reverse=True)

    msg = "ğŸ“¡ <b>ì›Œí¬ë© ë¦¬ì„œì¹˜ ë¸Œë¦¬í•‘</b>\n(ìµœê·¼ 48ì‹œê°„ / ìƒìœ„ 10ê±´)\n\n"

    if competitors:
        msg += "â”â”â”â”â”â”â”â”â”â”\n<b>ğŸ¢ ê²½ìŸì‚¬ íë¦„</b>\nâ”â”â”â”â”â”â”â”â”â”\n"
        for i, a in enumerate(competitors, 1):
            msg += format_item(i, a["tags"], a["title"], a["link"]) + "\n"

    if trends:
        msg += "â”â”â”â”â”â”â”â”â”â”\n<b>ğŸ“ˆ ê¸°ìˆ  íŠ¸ë Œë“œ</b>\nâ”â”â”â”â”â”â”â”â”â”\n"
        for i, a in enumerate(trends, 1):
            msg += format_item(i, a["tags"], a["title"], a["link"]) + "\n"

    send_telegram(msg)

if __name__ == "__main__":
    main()
