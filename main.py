import os
import re
import html
import json
import feedparser
import requests
from datetime import datetime, timedelta, timezone
from dateutil import parser as dateparser
from urllib.parse import urlparse, parse_qs

# =============================
# ê¸°ë³¸ ì„¤ì •
# =============================
MAX_ARTICLES = 10
TIME_WINDOW_HOURS = 48

# "ì´ë¯¸ ë³´ë‚¸ ê¸°ì‚¬" ì¤‘ë³µ ì œê±° (ì‹¤í–‰ ê°„ ìœ ì§€)
HISTORY_DAYS = 30
STATE_DIR = ".cache/walklab_radar"
STATE_FILE = os.path.join(STATE_DIR, "state.json")

KST = timezone(timedelta(hours=9))
now_kst = datetime.now(KST)
cutoff_kst = now_kst - timedelta(hours=TIME_WINDOW_HOURS)

TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
    raise RuntimeError("Missing TELEGRAM_BOT_TOKEN or TELEGRAM_CHAT_ID in environment variables.")

# =============================
# RSS ëª©ë¡ (ê²½ìŸì‚¬ + íŠ¸ë Œë“œ + ì‹¤ë£¨ì—£/ìì„¸ + EverEx)
# =============================
RSS_FEEDS = [
    # ===== ê²½ìŸì‚¬ =====
    'https://news.google.com/rss/search?q=("AIT+Studio"+OR+AITìŠ¤íŠœë””ì˜¤+OR+ì—ì´íŠ¸ìŠ¤íŠœë””ì˜¤)+(MediStep+OR+ë©”ë””ìŠ¤í…)+(gait+OR+ë³´í–‰)&hl=ko&gl=KR&ceid=KR:ko',
    'https://news.google.com/rss/search?q=("Angel+Robotics"+OR+ì—”ì ¤ë¡œë³´í‹±ìŠ¤)+("Angel+Legs"+OR+M20)+(gait+OR+rehabilitation)&hl=ko&gl=KR&ceid=KR:ko',
    'https://news.google.com/rss/search?q=("WIRobotics"+OR+ìœ„ë¡œë³´í‹±ìŠ¤)+(gait+OR+ì›¨ì–´ëŸ¬ë¸”)&hl=ko&gl=KR&ceid=KR:ko',
    'https://news.google.com/rss/search?q=(PediSol+OR+í˜ë””ì†”+OR+"Spina+Systems")+("smart+insole"+OR+ì¡±ì €ì••)&hl=ko&gl=KR&ceid=KR:ko',
    'https://news.google.com/rss/search?q=(Ochy)+(gait)&hl=en-US&gl=US&ceid=US:en',
    'https://news.google.com/rss/search?q=("LocoStep"+OR+"ExaMD")+(gait)&hl=en-US&gl=US&ceid=US:en',
    'https://news.google.com/rss/search?q=("OneStep")+(gait+OR+rehabilitation)&hl=en-US&gl=US&ceid=US:en',

    # ===== ì¶”ê°€ ê²½ìŸì‚¬: EverEx (êµ­ë¬¸ 2 + ì˜ë¬¸ 2) =====
    'https://news.google.com/rss/search?q=("ì—ë²„ì—‘ìŠ¤"+OR+"EverEx")+(íˆ¬ì+OR+ì‹œë¦¬ì¦ˆ+OR+ì„ìƒ+OR+ë³‘ì›+OR+MOU+OR+ì œíœ´+OR+ë³´í—˜+OR+ìˆ˜ê°€+OR+ë””ì§€í„¸ì¹˜ë£Œê¸°ê¸°+OR+DTx+OR+ê³¼ì œ+OR+í•´ì™¸ì§„ì¶œ)&hl=ko&gl=KR&ceid=KR:ko',
    'https://news.google.com/rss/search?q=("ì—ë²„ì—‘ìŠ¤"+OR+"EverEx")+(AI+OR+ì˜ìƒ+OR+ë¹„ì „+OR+ë¶„ì„+OR+ìš´ë™ì½”ì¹­+OR+ì¬í™œí”Œë«í¼+OR+ìì„¸+OR+ì‹¤ë£¨ì—£+OR+êµ°ì§‘)&hl=ko&gl=KR&ceid=KR:ko',
    'https://news.google.com/rss/search?q=("EverEx")+(funding+OR+investment+OR+clinical+OR+hospital+OR+insurance+OR+DTx+OR+expansion+OR+partnership)&hl=en-US&gl=US&ceid=US:en',
    'https://news.google.com/rss/search?q=("EverEx")+("digital+rehabilitation"+OR+"AI+therapy"+OR+"motion+analysis"+OR+"pose+estimation"+OR+"exercise+platform")&hl=en-US&gl=US&ceid=US:en',

    # ===== íŠ¸ë Œë“œ 4ì¶• =====
    'https://news.google.com/rss/search?q=("smartphone+video+gait"+OR+"video+gait+analysis")+(clinical+OR+validation)&hl=en-US&gl=US&ceid=US:en',
    'https://news.google.com/rss/search?q=("gait+digital+biomarker"+OR+"mobility+data")+(insurance+OR+underwriting)&hl=en-US&gl=US&ceid=US:en',
    'https://news.google.com/rss/search?q=("fall+prevention"+OR+"fall+risk")+(elderly+OR+seniors)&hl=en-US&gl=US&ceid=US:en',
    'https://news.google.com/rss/search?q=("gait+diabetes"+OR+"gait+Parkinson")+(AI+OR+model)&hl=en-US&gl=US&ceid=US:en',

    # ===== ì¶”ê°€: ìì„¸(ì‹¤ë£¨ì—£) 1ì¶•(ì˜/êµ­ë¬¸ í¬í•¨) =====
    'https://news.google.com/rss/search?q=("silhouette+analysis"+OR+"silhouette-based"+OR+"silhouette+score"+OR+clustering)+("posture"+OR+"pose+estimation"+OR+"motion+analysis"+OR+biomechanics)+(gait+OR+rehabilitation+OR+healthcare+OR+clinical)&hl=en-US&gl=US&ceid=US:en',
    'https://news.google.com/rss/search?q=(ì‹¤ë£¨ì—£+OR+ìì„¸+OR+í¬ì¦ˆì¶”ì •+OR+êµ°ì§‘ë¶„ì„)+(ë³´í–‰+OR+ì¬í™œ+OR+í—¬ìŠ¤ì¼€ì–´+OR+ì˜ë£Œ)+-íŒ¨ì…˜+-ì˜ë¥˜+-ì‚¬ì§„&hl=ko&gl=KR&ceid=KR:ko',
]

# =============================
# íƒœê·¸ ë¶„ë¥˜
# =============================
COMPANY_KEYWORDS = [
    "AIT Studio", "AITìŠ¤íŠœë””ì˜¤", "ì—ì´íŠ¸ìŠ¤íŠœë””ì˜¤", "MediStep", "ë©”ë””ìŠ¤í…",
    "Angel Robotics", "ì—”ì ¤ë¡œë³´í‹±ìŠ¤", "Angel Legs", "M20",
    "WIRobotics", "ìœ„ë¡œë³´í‹±ìŠ¤",
    "Spina Systems", "ìŠ¤í”¼ë‚˜ì‹œìŠ¤í…œì¦ˆ", "PediSol", "í˜ë””ì†”",
    "Ochy", "LocoStep", "ExaMD", "OneStep",
    "EverEx", "ì—ë²„ì—‘ìŠ¤"
]

TAG_RULES = {
    "ğŸ’°íˆ¬ì": ["funding", "series", "investment", "raises", "íˆ¬ì", "ì‹œë¦¬ì¦ˆ"],
    "ğŸ¤ì œíœ´": ["partnership", "collaboration", "mou", "ì œíœ´", "í˜‘ì•½", "mou"],
    "ğŸ¥ì„ìƒ": ["clinical", "trial", "fda", "validation", "hospital", "ì„ìƒ", "ë³‘ì›"],
    "ğŸ›ê³µê³µ": ["government", "city", "public", "ì •ë¶€", "ì§€ìì²´", "ê³µê³µ"],
    "ğŸ›¡ë³´í—˜": ["insurance", "underwriting", "payer", "ë³´í—˜", "ìˆ˜ê°€"],
    "ğŸ“±ì˜ìƒê¸°ë°˜": ["smartphone", "video", "camera", "markerless", "ì˜ìƒ", "ë¹„ì „", "ì¹´ë©”ë¼"],
    "ğŸ§ì‹¤ë£¨ì—£": [
        "silhouette analysis", "silhouette-based", "silhouette score",
        "clustering", "cluster analysis",
        "posture", "pose estimation", "biomechanics",
        "ì‹¤ë£¨ì—£", "ìì„¸", "í¬ì¦ˆì¶”ì •", "êµ°ì§‘ë¶„ì„"
    ]
}

# =============================
# ìœ í‹¸
# =============================
def send_telegram(message: str):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }
    r = requests.post(url, data=payload, timeout=20)
    r.raise_for_status()

def normalize_title(title: str) -> str:
    return re.sub(r"[^0-9a-zA-Zê°€-í£]+", "", title).lower()

def parse_published_kst(entry):
    if hasattr(entry, "published"):
        try:
            dt = dateparser.parse(entry.published)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt.astimezone(KST)
        except Exception:
            return None
    return None

def classify_tags(title: str, summary: str):
    text = (title + " " + summary).lower()
    tags = []

    for kw in COMPANY_KEYWORDS:
        if kw.lower() in text:
            tags.append("ğŸ¢ê²½ìŸì‚¬")
            break

    for tag, kws in TAG_RULES.items():
        for k in kws:
            if k.lower() in text:
                tags.append(tag)
                break

    return tags

def calc_priority(tags):
    score = 0
    if "ğŸ¢ê²½ìŸì‚¬" in tags:
        score += 100
    if "ğŸ’°íˆ¬ì" in tags:
        score += 30
    if "ğŸ¥ì„ìƒ" in tags:
        score += 30
    if "ğŸ¤ì œíœ´" in tags:
        score += 20
    if "ğŸ›¡ë³´í—˜" in tags:
        score += 15
    if "ğŸ›ê³µê³µ" in tags:
        score += 15
    if "ğŸ§ì‹¤ë£¨ì—£" in tags:
        score += 10
    if "ğŸ“±ì˜ìƒê¸°ë°˜" in tags:
        score += 8
    return score

def extract_original_url(url: str) -> str:
    """
    Google News RSS ë§í¬ì— url= íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì›ë¬¸ URLë¡œ ë°”ê¿‰ë‹ˆë‹¤.
    (ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜)
    """
    try:
        parsed = urlparse(url)
        qs = parse_qs(parsed.query)
        if "url" in qs and qs["url"]:
            return qs["url"][0]
    except Exception:
        pass
    return url

def format_item(i: int, tags, title: str, link: str) -> str:
    safe_title = html.escape(title)
    safe_link = html.escape(link)
    tag_text = " ".join(tags) if tags else ""
    return f"{i}. {tag_text}\n<a href=\"{safe_link}\">{safe_title}</a>\n"

# =============================
# ìƒíƒœ(íˆìŠ¤í† ë¦¬) ì €ì¥/ë¡œë“œ: 30ì¼
# =============================
def load_state():
    os.makedirs(STATE_DIR, exist_ok=True)
    if not os.path.exists(STATE_FILE):
        return {"sent": []}

    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "sent" not in data or not isinstance(data["sent"], list):
            return {"sent": []}
        return data
    except Exception:
        return {"sent": []}

def prune_state(state):
    keep_after = now_kst - timedelta(days=HISTORY_DAYS)
    pruned = []
    for item in state.get("sent", []):
        try:
            ts = dateparser.parse(item.get("sent_at"))
            if ts.tzinfo is None:
                ts = ts.replace(tzinfo=timezone.utc)
            ts_kst = ts.astimezone(KST)
            if ts_kst >= keep_after:
                pruned.append(item)
        except Exception:
            # íŒŒì‹± ì‹¤íŒ¨ í•­ëª©ì€ ë²„ë¦¼
            continue
    state["sent"] = pruned
    return state

def save_state(state):
    os.makedirs(STATE_DIR, exist_ok=True)
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def build_history_sets(state):
    url_set = set()
    title_set = set()
    for item in state.get("sent", []):
        u = (item.get("url") or "").strip()
        t = (item.get("title_norm") or "").strip()
        if u:
            url_set.add(u)
        if t:
            title_set.add(t)
    return url_set, title_set

# =============================
# ë©”ì¸
# =============================
def main():
    # íˆìŠ¤í† ë¦¬ ë¡œë“œ + 30ì¼ í”„ë£¨ë‹
    state = prune_state(load_state())
    sent_url_set, sent_title_set = build_history_sets(state)

    articles = []
    seen_links = set()
    seen_titles = set()

    for feed_url in RSS_FEEDS:
        feed = feedparser.parse(feed_url)

        for entry in getattr(feed, "entries", []):
            published_kst = parse_published_kst(entry)
            if not published_kst:
                continue
            if published_kst < cutoff_kst:
                continue

            raw_link = getattr(entry, "link", "").strip()
            title = getattr(entry, "title", "").strip()
            summary = getattr(entry, "summary", "")

            if not raw_link or not title:
                continue

            # ì›ë¬¸ ë§í¬ë¡œ ì •ê·œí™”(ì¤‘ë³µ íŒë‹¨ì˜ í•µì‹¬)
            link = extract_original_url(raw_link).strip()
            norm_title = normalize_title(title)

            # (ì‹¤í–‰ ë‚´ë¶€) ì¤‘ë³µ ì œê±°
            if link in seen_links or norm_title in seen_titles:
                continue

            # (ì‹¤í–‰ ê°„) ì´ë¯¸ ë³´ë‚¸ ê¸°ì‚¬ í•„í„°: URL + ì œëª©(ì •ê·œí™”) ê¸°ì¤€
            if link in sent_url_set or norm_title in sent_title_set:
                continue

            seen_links.add(link)
            seen_titles.add(norm_title)

            tags = classify_tags(title, summary)
            priority = calc_priority(tags)

            articles.append({
                "title": title,
                "title_norm": norm_title,
                "link": link,
                "tags": tags,
                "priority": priority,
                "published_kst": published_kst
            })

    # ìš°ì„ ìˆœìœ„/ìµœì‹ ìˆœ ì •ë ¬ í›„ ìƒìœ„ 10ê±´
    articles.sort(key=lambda x: (x["priority"], x["published_kst"]), reverse=True)
    top = articles[:MAX_ARTICLES]

    if not top:
        send_telegram("ğŸ“¡ ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ (ìµœê·¼ 48ì‹œê°„ / ì¤‘ë³µ ì œì™¸)")
        # í”„ë£¨ë‹ ê²°ê³¼ëŠ” ì €ì¥(íŒŒì¼ ì†ìƒ ëŒ€ë¹„)
        save_state(state)
        return

    # êµ¬ì—­ ë¶„ë¦¬(ìƒìœ„ 10ê±´ ì•ˆì—ì„œë§Œ)
    competitors = [a for a in top if "ğŸ¢ê²½ìŸì‚¬" in a["tags"]]
    trends = [a for a in top if "ğŸ¢ê²½ìŸì‚¬" not in a["tags"]]

    competitors.sort(key=lambda x: (x["priority"], x["published_kst"]), reverse=True)
    trends.sort(key=lambda x: (x["priority"], x["published_kst"]), reverse=True)

    msg = "ğŸ“¡ <b>ì›Œí¬ë© ë¦¬ì„œì¹˜ ë¸Œë¦¬í•‘</b>\n(ìµœê·¼ 48ì‹œê°„ / ì¤‘ë³µ ì œì™¸ / ìƒìœ„ 10ê±´)\n\n"

    if competitors:
        msg += "â”â”â”â”â”â”â”â”â”â”\n<b>ğŸ¢ ê²½ìŸì‚¬ íë¦„</b>\nâ”â”â”â”â”â”â”â”â”â”\n"
        for i, a in enumerate(competitors, 1):
            msg += format_item(i, a["tags"], a["title"], a["link"]) + "\n"

    if trends:
        msg += "â”â”â”â”â”â”â”â”â”â”\n<b>ğŸ“ˆ ê¸°ìˆ  íŠ¸ë Œë“œ</b>\nâ”â”â”â”â”â”â”â”â”â”\n"
        for i, a in enumerate(trends, 1):
            msg += format_item(i, a["tags"], a["title"], a["link"]) + "\n"

    # ì „ì†¡
    send_telegram(msg)

    # ì „ì†¡ ì„±ê³µí•œ í•­ëª©ë“¤ì„ íˆìŠ¤í† ë¦¬ì— ê¸°ë¡ (30ì¼ ìœ ì§€)
    sent_at = now_kst.isoformat()
    for a in top:
        state["sent"].append({
            "url": a["link"],
            "title_norm": a["title_norm"],
            "sent_at": sent_at
        })

    # ì €ì¥ (actions/cacheê°€ ë‹¤ìŒ ì‹¤í–‰ì— ë³µì›)
    save_state(state)

if __name__ == "__main__":
    main()
